{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Basic\n",
    "Basic Spark CRUD operations\n",
    "RDD & Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Data RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col, max as max_\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import subprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('spark-basic')\n",
    "sc=SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSlices / number of partititions for the rdd is 2.\n",
    "num_rdd = sc.parallelize([1,2,3,4,5], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(num_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper\n",
    "\n",
    "num_rdd.map(lambda x: x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper & reducer\n",
    "\n",
    "num_rdd.map(lambda x: x*x).reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSlices = 3\n",
    "num_rdd = sc.parallelize([1,2,3,4,5,6], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "num_rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rdd = sc.parallelize(['Mathematics', 'Science', 'Mathematics', 'History', 'Biology', 'Science'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct\n",
    "list_rdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "set_2 = sc.parallelize([6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "set_1.intersection(set_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([1,2,4,9,16,25])\n",
    "y = sc.parallelize([1,3,5,7,9,11,13])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union\n",
    "x.union(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct & union\n",
    "x.union(y).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map & zip\n",
    "a = list_rdd.map(lambda x: len(x))\n",
    "list_rdd.zip(a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap\n",
    "num_rdd = sc.parallelize([1,2,3,4,5,6], 3)\n",
    "num_rdd.flatMap(lambda x: range(1, x+1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd = sc.parallelize([10, 15, 20], 2)\n",
    "num_rdd.flatMap(lambda x: [x, x, x]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys\n",
    "a = list_rdd.map(lambda x: (len(x), x))\n",
    "a.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartesian\n",
    "x = sc.parallelize([1,2,4])\n",
    "y = sc.parallelize([7,9,11])\n",
    "x.cartesian(y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupBy\n",
    "a = list_rdd.groupBy(lambda x: len(x)).collect()\n",
    "for (x, y) in a:\n",
    "    print (x)\n",
    "    for i in y:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([(x,sorted(y)) for(x,y) in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyBy\n",
    "a = sc.parallelize(['blue', 'green', 'orange'])\n",
    "b = sc.parallelize(['black', 'white', 'grey'])\n",
    "c = a.keyBy(lambda x: len(x))\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = b.keyBy(lambda x: len(x))\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.join(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leftOuterJoin -- include the left of the operation, this case is c object\n",
    "c.leftOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rightOuterJoin -- include the right of the operation, this case is d object\n",
    "c.rightOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullOuterJoin -- include both left & right of the operation, this case c & d objects\n",
    "c.fullOuterJoin(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "f = a.union(b)\n",
    "f.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = f.map(lambda x: (len(x), x))\n",
    "g.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = g.reduceByKey(lambda x, y: x + '/' + y)\n",
    "h.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeSample (withReplacement, num, [seed])\n",
    "num_rdd = sc.parallelize([10, 4, 5, 3, 11, 2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd.takeSample(True, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame DF\n",
    "Column-oriented data organization --> make things easier to understand\n",
    "\n",
    "do this for the following exercises:\n",
    "\n",
    "```\n",
    "hdfs dfs -mkdir /tmp\n",
    "hdfs dfs -copyFromLocal authors.json /tmp\n",
    "hdfs dfs -copyFromLocal authors_missing.json /tmp\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must create sparksession from sc to avoid this error: \n",
    "# 'PipelinedRDD' object has no attribute 'toDF' in PySpark\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['yellow', 'black', 'white', 'blue', 'green', 'brown', 'pink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df = sc.parallelize(colors).map(lambda x: (x, len(x))).toDF(['color', 'length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from json -- remember to do this hdfs dfs -copyFromLocal authors.json /tmp/authors.json\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json('/tmp/authors.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.drop('length').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.toJSON().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.filter(color_df.length.between(4,5)).select(color_df.color.alias('mid_length')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.filter(color_df.length > 4).filter(color_df[0] != 'white').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.sort('color').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.filter(color_df['length'] > 4).sort('length', 'color', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.orderBy('length', 'color').take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.sort(color_df.length.desc(), color_df.color.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_df.groupBy('length').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load file\n",
    "df1 = ss.read.json('/tmp/authors_missing.json')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.dropna()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example from Airline Performance data\n",
    "\n",
    "Need to prepare airline traffic data by running the following command at `cisc-525-util` directory\n",
    "\n",
    "```\n",
    "cd ~/cisc525/cisc-525-util\n",
    "./prepare-hadoop-data.bash\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"performance-app\").config(\"spark.config.option\", \"value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\\\n",
    "                     StructField('Year', IntegerType(), True),\\\n",
    "                     StructField('Month', IntegerType(), True),\\\n",
    "                     StructField('DayOfMonth', IntegerType(), True),\\\n",
    "                     StructField('DayOfWeek', IntegerType(), True),\\\n",
    "                     StructField('DepTime', IntegerType(), True),\\\n",
    "                     StructField('CRSDepTime', IntegerType(), True),\\\n",
    "                     StructField('ArrTime', IntegerType(), True),\\\n",
    "                     StructField('CRSArrTime', IntegerType(), True),\\\n",
    "                     StructField('UniqueCarrier', StringType(), True),\\\n",
    "                     StructField('FlightNum', IntegerType(), True),\\\n",
    "                     StructField('TailNum', StringType(), True),\\\n",
    "                     StructField('ActualElapsedTime', StringType(), True),\\\n",
    "                     StructField('CRSElapsedTime', StringType(), True),\\\n",
    "                     StructField('AirTime', StringType(), True),\\\n",
    "                     StructField('ArrDelay', StringType(), True),\\\n",
    "                     StructField('DepDelay', IntegerType(), True),\\\n",
    "                     StructField('Origin', StringType(), True),\\\n",
    "                     StructField('Dest', StringType(), True),\\\n",
    "                     StructField('Distance', StringType(), True),\\\n",
    "                     StructField('TaxiIn', StringType(), True),\\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 'hdfs://localhost:9000/user/student/airline/1987.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').option('header', 'true').schema(schema).load(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('uniquecarrier').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('origin').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('dest').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for delay that is greater than 40.\n",
    "df.filter(df['depdelay'] > 40).groupBy('dest').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('uniquecarrier', 'origin', 'dest').orderBy('origin').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df['uniquecarrier'] == 'UA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupBy('uniquecarrier').mean('depdelay').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache\n",
    "df.createOrReplaceTempView('flights')\n",
    "spark.catalog.cacheTable('flights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select uniquecarrier, origin, dest, depdelay from flights where depdelay > 40 order by depdelay desc limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select uniquecarrier, avg(depdelay) from flights group by uniquecarrier').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
